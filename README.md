# Sistema de Resiliencia Operativa y DRP (Plan de Recuperaci贸n ante Desastres)

----
## Descripci贸n del Proyecto

### El Problema: 
En la administraci贸n de sistemas tradicional, los backups suelen fallar silenciosamente. Los scripts de copia (`cp`, `rsync`) no garantizan la consistencia si la base de datos est谩 escribiendo en ese momento, y la recuperaci贸n manual ("DRP en papel") es lenta y propensa a errores humanos bajo presi贸n.

> **Si un backup no ha sido probado mediante una restauraci贸n, no existe.**

### La Soluci贸n
Este proyecto implementa un **Sistema de Resiliencia Automatizada** dise帽ado para cumplir con los est谩ndares de **Continuidad Operacional**. Transformamos el DRP en c贸digo ejecutable para garantizar que la recuperaci贸n sea:
1.  **Consistente:** Uso de **LVM Snapshots** para "congelar" el estado del disco en milisegundos, asegurando que la base de datos (MariaDB) nunca se copie en un estado corrupto.
2.  **Eficiente:** Implementaci贸n de **Restic** para backups incrementales con deduplicaci贸n. Solo se transmiten y almacenan los bytes que han cambiado, reduciendo el uso de red y almacenamiento.
3.  **Gestionada:** Aplicaci贸n de pol铆ticas de retenci贸n **GFS (Grandfather-Father-Son)** autom谩ticas para mantener copias hist贸ricas sin saturar el almacenamiento.
4.  **Inmortal:** Orquestaci贸n con **Ansible** para automatizar la resurrecci贸n completa del servicio en minutos, eliminando el factor humano durante la crisis.

---

##  Arquitectura de la Soluci贸n

El sistema se distribuye en **4 Nodos L贸gicos** interconectados, dise帽ados para simular un entorno de producci贸n real donde los servicios (App/DB), el almacenamiento (Backups) y la gesti贸n (Control) est谩n desacoplados para garantizar la supervivencia de los datos incluso si los servidores principales son comprometidos.

# AQUI PONDREMOS LA TOPOLOGIA


| Nodo | Rol | Funci贸n Cr铆tica |
| :--- | :--- | :--- |
| **VM1** | `B贸veda (Storage)` | Almacenamiento inmutable de backups (MinIO). Act煤a como "caja negra" externa. |
| **VM2** | `App Node` | Servidor Web (Nginx/Apache). Representa la cara visible del negocio. |
| **VM3** | `DB Node` | Base de Datos (MariaDB) sobre l煤menes LVM. Es el activo m谩s valioso. |
| **VM4** | `Cerebro (Control)` | Nodo de gesti贸n desde donde Ansible ejecuta la recuperaci贸n autom谩tica. |

---

## Tecnolog铆as Seleccionadas

### 1. Motor de Backup: Restic

* **驴Por qu茅?** **Restic** utiliza una arquitectura de **"Chunk-Based Deduplication"** (Deduplicaci贸n basada en fragmentos).
* **Eficiencia:** Si cambias 1 MB en una base de datos de 10 GB, Restic solo transfiere y guarda ese 1 MB. Esto cumple con el requisito de **estrategia incremental** sin la complejidad de gestionar cadenas de incrementales fr谩giles.
* **Seguridad:** Todo dato que sale del servidor es cifrado con **AES-256** (Cifrado sim茅trico fuerte con una longitud clave de 256 bits) antes de tocar la red.

### 2. Consistencia de Datos: LVM (Logical Volume Manager)
> *Requisito: Integridad en Caliente*

* **El Desaf铆o:** Copiar los archivos de una base de datos mientras est谩 encendida resulta en backups corruptos e inutilizables.
* **La Soluci贸n:** Utilizamos **Snapshots LVM**. Esto "congela" el sistema de archivos en el tiempo exacto en milisegundos, permitiendo a Restic copiar los datos est谩ticos mientras la base de datos sigue recibiendo escrituras en un espacio temporal.

### 3. Planificaci贸n: Systemd Timers

* **La Mejora:** **Systemd** maneja dependencias (ej: *"no inicies el backup si no hay red"*), reintentos autom谩ticos si falla la conexi贸n, y registro de logs centralizado (`journalctl`). Es vital para la **Automatizaci贸n** robusta.

### 4. Orquestaci贸n DRP: Ansible
> *Requisito: Automatizaci贸n de la Restauraci贸n*

* **驴Por qu茅?** Un Plan de Recuperaci贸n ante Desastres (DRP) documentado en papel es lento y propenso a error humano durante una crisis.
* **La Soluci贸n:** Transformamos el DRP en **Playbooks de Ansible**. Esto nos permite reconstruir el servicio, reinstalar dependencias y restaurar los datos con un solo comando, reduciendo el **RTO (Recovery Time Objective)** de horas a minutos.

---
##  Estrategia de Retenci贸n de Datos

### Perfil Producci贸n (GFS en un entorno serio)
En un entorno empresarial real, aplicamos el esquema est谩ndar **Grandfather-Father-Son** para cumplir con auditor铆as y recuperaci贸n a largo plazo:

* **Son (Diario):** `--keep-daily 7` (Mantiene 1 backup por d铆a durante una semana).
* **Father (Semanal):** `--keep-weekly 4` (Mantiene 1 por semana durante un mes).
* **Grandfather (Mensual):** `--keep-monthly 6` (Mantiene 1 por mes durante medio a帽o).

### Perfil Demostraci贸n (Para la demostracion para la feria)
> Debido a la naturaleza ef铆mera del evento (ciclos de vida de minutos), hemos ajustado el "cron贸metro" para operar en **Alta Frecuencia**:

* **Frecuencia de Backup:** Cada **60 segundos** (Systemd Timer).
* **Pol铆tica de Retenci贸n:** `keep-last 20`.
    * *Justificaci贸n:* Esto nos permite viajar en el tiempo minuto a minuto durante la presentaci贸n, mostrando cambios inmediatos sin esperar d铆as o semanas.



> *Esto demuestra la capacidad de Restic para gestionar el ciclo de vida de los datos sin intervenci贸n humana, escalando desde minutos (demo) hasta a帽os (producci贸n).*

---

##  Protocolo de Integridad 
Un riesgo cr铆tico en sistemas automatizados es que el backup autom谩tico se ejecute *justo despu茅s* de un incidente destructivo, guardando un estado "vac铆o" o corrupto como el m谩s reciente (`latest`).

**Nuestra Soluci贸n: Restauraci贸n por ID Inmutable.**
En lugar de restaurar ciegamente la etiqueta `latest`, nuestro **Men煤 de Control (Ansible)** permite al operador seleccionar un **Snapshot ID** espec铆fico.
1.  El sistema sigue haciendo backups (incluso del desastre), lo cual sirve como registro forense.
2.  El operador visualiza la l铆nea de tiempo.
3.  Se selecciona el punto de restauraci贸n *previo* al incidente (T-1 minuto).

> **Resiliencia no es solo guardar datos, es saber qu茅 versi贸n de la verdad recuperar.**

***

# Fases de Implementaci贸n

Para garantizar la replicabilidad y el 茅xito del **Sistema de Resiliencia Operativa**, hemos dividido la ejecuci贸n en 9 fases estrat茅gicas. Cada fase construye una capa de funcionalidad sobre la anterior, desde la infraestructura f铆sica hasta la interfaz de usuario final.

-----

## Fase 1: Aprovisionamiento de Infraestructura Base
**Descripci贸n:**
Consiste en la creaci贸n y configuraci贸n inicial de los 4 Nodos Virtuales (VMs) que compondr谩n el sistema. Se establecen los recursos de hardware virtual (CPU, RAM, Disco) y se instala el sistema operativo base (Ubuntu Server).

**Objetivo y Aporte:**
* Establecer los cimientos del sistema.
* Segregar funciones: Separar la l贸gica de negocio (App/DB), el almacenamiento (B贸veda) y la gesti贸n (Control) para evitar un "Punto nico de Fallo" (SPOF).

### Paso 1: Definici贸n del Hardware Virtual

** Descripci贸n:**
Vamos a configurar "el chasis" de nuestras 4 m谩quinas virtuales en **VirtualBox**. Dado que son 4 computadoras f铆sicas, se creara **una VM en cada laptop**.

Configura cada VM con los siguientes par谩metros cr铆ticos:

| VM | Rol | RAM | CPU | Disco Principal (OS) | Discos Extra | Red |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| **VM1** | `minio-vault` | 2048 MB | 2 | 25 GB (VDI Din谩mico) | **+20 GB** (VDI para Backups) | NAT |
| **VM2** | `app-node` | 1024 MB | 1 | 25 GB (VDI Din谩mico) | - | NAT |
| **VM3** | `db-node` | 2048 MB | 2 | 25 GB (VDI Din谩mico) | **+10 GB** (VDI para LVM) | NAT |
| **VM4** | `drp-control` | 1024 MB | 1 | 25 GB (VDI Din谩mico) | - | NAT |

> **Nota Cr铆tica:** En la configuraci贸n de Red de VirtualBox, debe estar en **"NAT"**. Esto a铆sla la VM pero le da salida a internet usando la IP de la maquina fisica.

Establece los l铆mites f铆sicos de nuestros servidores. Asignar discos secundarios a la VM1 y VM3 es vital porque simula la separaci贸n profesional entre "Sistema Operativo" y "Datos Cr铆ticos". Si el OS explota, el disco de datos sobrevive.

Tener los 4 contenedores virtuales listos para recibir el sistema operativo, con el almacenamiento f铆sico segregado correctamente para cumplir con los requisitos de LVM y Backups.

---
Recibido. Cambio de variables aceptado.
Usar usuarios distintos (`minio`, `app`, `db`, `drp`) es una pr谩ctica excelente para auditor铆a (sabes qui茅n rompi贸 qu茅) y refleja la realidad de un equipo distribuido.

Procedemos con el **Paso 2**. Aqu铆 es donde el software toca el metal.

***

### Paso 2: Instalaci贸n del OS y Definici贸n de Identidad

**Descripci贸n:**
Instalaci贸n del sistema operativo **Ubuntu Server (LTS)** en cada una de las 4 m谩quinas virtuales.

**Objetivo:**
Tener 4 servidores Linux arrancando, con acceso a internet y servicio SSH activo.

Al instalar y llenar los campos de
Your name:  
Your servers name:  
Pick a username:  
choose a password:  
nosotros para esta documentacion y el proyecto trabajeremos con:
| Rol | Your name | Your servers name | Pick a username | choose a password |
|---------|---------|---------|---------|---------|
|`minio-vault`| Admin Vault | minio-vault | admin-vault | 1234 |
|`app-node`| Admin App | app-node | admin-app | 4321 |
|`db-node`| Admin DB | db-node | admin-db | 5678 |
|`drp-control`| Admin DRP | drp-control | admin-drp | 8765 |
> En un entorno real se debe tener discrecion con las contrase帽as  
> En este caso usaremos estos datos, pero si se desea replicar se debe ajustar algunos comandos a sus datos.














-----

## Fase 2: Implementaci贸n de Red de Malla (Overlay Network)
**Descripci贸n:**
Despliegue de una red privada virtual (VPN de malla) utilizando **Tailscale/ZeroTier**. Esto crea una capa de red abstracta sobre la infraestructura f铆sica, permitiendo que las m谩quinas se comuniquen de forma segura y encriptada sin depender de la configuraci贸n del router local (Wi-Fi de la feria o laboratorio).

**Objetivo y Aporte:**
* **Portabilidad Total:** El sistema funciona id茅nticamente en el laboratorio, en una feria p煤blica o en Internet.
* **Independencia de IP:** Uso de *MagicDNS* para resolver nombres (`db-node`, `app-node`) en lugar de depender de IPs est谩ticas fr谩giles.
* **Seguridad:** Todo el tr谩fico entre nodos viaja cifrado, inmune a espionaje en redes p煤blicas.

-----

## Fase 3: Despliegue de la B贸veda Inmutable (Object Storage)
**Descripci贸n:**
Instalaci贸n y configuraci贸n de **MinIO** (compatible con Amazon S3) en un entorno contenerizado (Docker). Se configuran las pol铆ticas de acceso, usuarios de servicio y persistencia de datos en disco.

**Objetivo y Aporte:**
* Crear un repositorio centralizado y desacoplado para los backups.
* Simular una arquitectura de nube real (Cloud-Native) en un entorno local.
* Garantizar que si los servidores de aplicaci贸n son destruidos, los datos de respaldo permanezcan intactos en un "b煤nker" aislado.

-----

## Fase 4: Configuraci贸n de Servicios Cr铆ticos (Las V铆ctimas)
**Descripci贸n:**
Puesta en marcha de los servicios que simulan la operaci贸n del negocio:
1.  **Servidor Web (App Node):** Nginx/Apache sirviendo una aplicaci贸n de demostraci贸n.
2.  **Base de Datos (DB Node):** MariaDB configurada para transacciones.
3.  **Acceso P煤blico:** Configuraci贸n de *Port Forwarding* para permitir acceso desde dispositivos externos (m贸viles de la audiencia).

**Objetivo y Aporte:**
* Proveer los activos de informaci贸n que ser谩n protegidos.
* Demostrar la accesibilidad del servicio para el usuario final antes del "desastre".

-----

## Fase 5: Integridad de Datos y Vol煤menes L贸gicos (LVM)
**Descripci贸n:**
Implementaci贸n de **LVM (Logical Volume Manager)** en el nodo de Base de Datos. Se migra el almacenamiento de MySQL/MariaDB a un volumen l贸gico dedicado, permitiendo la gesti贸n avanzada del disco.

**Objetivo y Aporte:**
* **Atomicidad:** Habilitar la capacidad de tomar *Snapshots* (fotograf铆as instant谩neas) del disco.
* **Consistencia:** Asegurar que los backups de la base de datos se realicen sin corromper la informaci贸n, incluso si el sistema est谩 recibiendo escrituras en ese milisegundo.

-----

## Fase 6: Configuraci贸n del Motor de Resiliencia (Restic)
**Descripci贸n:**
Instalaci贸n e inicializaci贸n de **Restic** en los nodos de aplicaci贸n y base de datos. Se configuran las variables de entorno para conectar con la B贸veda (MinIO) y se definen las claves de encriptaci贸n (AES-256).

**Objetivo y Aporte:**
* **Eficiencia:** Implementar la deduplicaci贸n de datos. Solo se almacenan los bloques que han cambiado, ahorrando espacio y ancho de banda.
* **Seguridad:** Garantizar que ning煤n dato salga del servidor sin estar cifrado (Encryption at Rest & in Transit).

-----

## Fase 7: Automatizaci贸n y Planificaci贸n de Alta Frecuencia
**Descripci贸n:**
Programaci贸n de **Systemd Timers** y Servicios (`.service` y `.timer`) para ejecutar los scripts de backup autom谩ticamente. Se define la estrategia de retenci贸n (GFS) adaptada al evento (retenci贸n de minutos/horas).

**Objetivo y Aporte:**
* Eliminar el error humano en la ejecuci贸n de backups.
* Establecer un **RPO (Recovery Point Objective)** cercano a cero, realizando copias de seguridad cada minuto de forma transparente.
* Gestionar el ciclo de vida de los datos (borrado autom谩tico de backups obsoletos).

-----

## Fase 8: Orquestaci贸n de Recuperaci贸n (DRP como C贸digo)
**Descripci贸n:**
Desarrollo de Playbooks de **Ansible** en el nodo de Control. Estos scripts contienen la l贸gica para detener servicios, desmontar discos, descargar copias de seguridad desde la B贸veda y restaurar el sistema a un estado operativo.

**Objetivo y Aporte:**
* **Reducci贸n del RTO (Recovery Time Objective):** Pasar de horas de restauraci贸n manual a segundos de restauraci贸n autom谩tica.
* **Idempotencia:** Asegurar que el proceso de recuperaci贸n sea repetible y libre de errores bajo presi贸n.

-----

## Fase 9: Interfaz de Demostraci贸n y Control Visual
**Descripci贸n:**
Creaci贸n de scripts interactivos (Men煤 de Mando) y monitores de estado en tiempo real. Esto permite al operador ejecutar ataques simulados y restauraciones quir煤rgicas seleccionando IDs espec铆ficos de backups.

**Objetivo y Aporte:**
* Hacer visible lo invisible: Permitir que la audiencia "vea" los backups ocurriendo en tiempo real.
* Facilitar la operaci贸n durante la feria, abstrayendo la complejidad de los comandos de terminal en un men煤 intuitivo.

-----
